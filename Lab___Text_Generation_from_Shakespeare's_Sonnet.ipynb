{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JChG9Gc_-TlH"
      },
      "source": [
        "#Lab | Text Generation from Shakespeare's Sonnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmaT_f18-B9_",
        "outputId": "a2a92d99-e0dd-4b65-81db-adc7c451491d"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlUqmId0-gj0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b93W838F-jCJ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "url = 'https://raw.githubusercontent.com/martin-gorner/tensorflow-rnn-shakespeare/master/shakespeare/sonnets.txt'\n",
        "resp = requests.get(url)\n",
        "with open('sonnets.txt', 'wb') as f:\n",
        "    f.write(resp.content)\n",
        "\n",
        "data = open('sonnets.txt').read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVo2rELE-l2h",
        "outputId": "939acdaa-4fa5-4a17-d8c4-0c499378d137"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Step 1: Initialize a tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Step 2: Fit the tokenizer on your corpus\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "# To see the results:\n",
        "word_index = tokenizer.word_index\n",
        "print(f\"Number of unique tokens: {len(word_index)}\")\n",
        "print(\"First 10 words and their indices:\", list(word_index.items())[:10])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "print(\"First 3 sequences:\", sequences[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pdzANyh-oNh",
        "outputId": "be0d5593-ac0c-4396-e739-5a6c83505027"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "# Assuming you have already fit the tokenizer on the corpus as shown in previous examples\n",
        "\n",
        "# Step 2: Calculate the Vocabulary Size\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(f\"The total number of unique words in the corpus is: {total_words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvexdGFk-qx4",
        "outputId": "e1be7617-f764-4c2d-cb4f-319ae0bc4cfc"
      },
      "outputs": [],
      "source": [
        "input_sequences = []\n",
        "\n",
        "\n",
        "\n",
        "for line in corpus:\n",
        "\n",
        "    # Convert each line into a sequence of integers\n",
        "\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "    for i in range(1, len(token_list)):\n",
        "\n",
        "        # Generate n-grams for each sequence\n",
        "\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "\n",
        "\n",
        "# Print first few sequences for verification\n",
        "\n",
        "print(input_sequences[:5])  # Show the first 5 n-gram sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl7JJqST-uJP",
        "outputId": "79207a4e-1802-4650-c31b-dc9f081dccd5"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Assuming input_sequences is already defined and contains your sequences\n",
        "# For example, let's say you have a list of sequences like this:\n",
        "# input_sequences = [[1, 2, 3], [4, 5], [6, 7, 8, 9], ...]\n",
        "\n",
        "# Step 1: Calculate the length of the longest sequence\n",
        "max_sequence_len = max(len(seq) for seq in input_sequences)\n",
        "\n",
        "# Step 2: Pad the sequences\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Convert to a numpy array (though `pad_sequences` already returns a numpy array)\n",
        "input_sequences = np.array(input_sequences)\n",
        "\n",
        "# Print to verify\n",
        "print(f\"The maximum sequence length is: {max_sequence_len}\")\n",
        "print(\"Padded sequences:\\n\", input_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOgbg4Oo-xf_",
        "outputId": "400731ce-24d0-4e44-a363-5c8a1d40ac35"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "# Assuming input_sequences is a numpy array containing your padded sequences\n",
        "\n",
        "# Initialize lists to store predictors and labels\n",
        "predictors = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over each sequence\n",
        "for sequence in input_sequences:\n",
        "    predictors.append(sequence[:-1])  # All elements except the last one\n",
        "    labels.append(sequence[-1])       # The last element\n",
        "\n",
        "# Convert lists to numpy arrays for use in model training\n",
        "predictors = np.array(predictors)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Print to verify\n",
        "print(\"Predictors:\\n\", predictors)\n",
        "print(\"Labels:\\n\", labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2pU5WNK-1Bx",
        "outputId": "429fe4e6-e589-423b-b6d5-a6ad5deb5c2a"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "from tensorflow.keras.utils import to_categorical  # Import the one-hot encoding function\n",
        "\n",
        "# Assuming labels is a numpy array of your integer labels and total_words is defined as before\n",
        "\n",
        "# One-hot encode the labels; num_classes should equal total number of unique words (vocabulary size)\n",
        "labels = to_categorical(labels, num_classes=total_words)\n",
        "\n",
        "# Print the shape to verify; it should be (number of sequences, total_words)\n",
        "print(\"One-hot encoded labels shape:\", labels.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8zwffBp-4dg"
      },
      "source": [
        "#initialize the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "sggnc5k5-3jq",
        "outputId": "5cffc2db-1aad-42e1-abe6-beca287f5935"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Assuming total_words and max_sequence_len are already defined\n",
        "embedding_dim = 100\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential([\n",
        "    # Embedding Layer\n",
        "    Embedding(input_dim=total_words, output_dim=embedding_dim, input_length=max_sequence_len - 1),\n",
        "\n",
        "    # Bidirectional LSTM Layer\n",
        "    Bidirectional(LSTM(150, return_sequences=True)),\n",
        "\n",
        "    # Dropout Layer\n",
        "    Dropout(0.2), # regularization to minimize overfitting by dropping out 20% of neurons randomly during training\n",
        "\n",
        "    # LSTM Layer\n",
        "    LSTM(100),\n",
        "\n",
        "    # Dense Layer (Intermediate)\n",
        "    Dense(total_words // 2, activation='relu', kernel_regularizer=l2(0.01)),  # L2 regularization for preventing overfitting\n",
        "\n",
        "    # Dense Layer (Output)\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "# Print the model summary to verify the layers\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYNocGNS_Zf2",
        "outputId": "a4923a34-0451-4b26-9aea-1d01c257a6f4"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',  # Suitable for multi-class classification\n",
        "              optimizer='adam',                 # Efficient and commonly used optimizer\n",
        "              metrics=['accuracy'])             # Track accuracy during training\n",
        "\n",
        "# Verify the compilation\n",
        "print(\"Model compiled successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "n0mCLq5q_cg5",
        "outputId": "d8a6d303-65b5-4d73-a35a-de886c70a760"
      },
      "outputs": [],
      "source": [
        "model.build (input_shape=(None, max_sequence_len -1)) # Correct input_shape specification when building the model manually\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOM0UIIV_hRK"
      },
      "source": [
        "# Train module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoQl-9NU_lqR"
      },
      "source": [
        "250 epoche"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mMf0NG9t_fKY",
        "outputId": "b846150a-dc98-4e53-b7f3-d0ea84e29a6a"
      },
      "outputs": [],
      "source": [
        "# Assuming the model has been defined previously with layers added in the Sequential API.\n",
        "\n",
        "# Step 1: Compile the model\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',  # Loss function suitable for one-hot encoded labels\n",
        "    optimizer='adam',                 # Optimizer used for updating model parameters\n",
        "    metrics=['accuracy']              # Metric for evaluating model performance\n",
        ")\n",
        "\n",
        "# Make sure the predictors and labels are properly prepared\n",
        "# predictors should match the expected input shape (num_samples, max_sequence_len - 1)\n",
        "# labels should be one-hot encoded, matching the total number of classes (vocabulary size)\n",
        "\n",
        "# Step 2: Train the model\n",
        "history = model.fit(\n",
        "    predictors,\n",
        "    labels,\n",
        "    epochs=250,\n",
        "    batch_size=45,\n",
        "    validation_split=0.2,  # Optionally use a portion of data for validation\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Optionally, you can print or plot the history to check training progress\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting training accuracy\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history.get('val_accuracy', []), label='val_accuracy')  # Only if validation data is used\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yRsgIArGsVz"
      },
      "source": [
        "#Plot table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "jIpAhyebAEFA",
        "outputId": "58369b45-e9d2-40b2-dec8-755ffed08715"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'history' is the object returned by model.fit\n",
        "\n",
        "# Extract the accuracy and loss data\n",
        "accuracy = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "# If you used a validation split, you can plot validation metrics as well\n",
        "val_accuracy = history.history.get('val_accuracy', [])\n",
        "val_loss = history.history.get('val_loss', [])\n",
        "\n",
        "# Plot training accuracy and validation accuracy over epochs\n",
        "plt.figure(figsize=(12, 6))  # Set figure size\n",
        "\n",
        "# Plot training accuracy\n",
        "plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
        "plt.plot(accuracy, label='Training Accuracy')\n",
        "# Plot validation accuracy if available\n",
        "if val_accuracy:\n",
        "    plt.plot(val_accuracy, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Plot training loss and validation loss over epochs\n",
        "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
        "plt.plot(loss, label='Training Loss')\n",
        "# Plot validation loss if available\n",
        "if val_loss:\n",
        "    plt.plot(val_loss, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()  # Adjust subplots to fit the figure area.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9Zest7fzGpyK",
        "outputId": "a5685268-2c01-4da3-dfa4-261334fc582f"
      },
      "outputs": [],
      "source": [
        "# prompt: Generate text with the model based on a seed text\n",
        "# Now you will create two variables :\n",
        "# seed_text = 'Write the text you want the model to use as a starting point to generate the next words'\n",
        "# next_words = number_of_words_you_want_the_model_to_generate\n",
        "# Please change number_of_words_you_want_the_model_to_generate by an actual integer.\n",
        "# # Your code here :\n",
        "\n",
        "seed_text = \"tell me something about Shakspear's sonnets.\"\n",
        "next_words = 20\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "seed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yP5AbBp5QHXI",
        "outputId": "65455ae4-8f81-499b-9cc8-1e4783000574"
      },
      "outputs": [],
      "source": [
        "# prompt: Now create a loop that runs based on the next_words variable and generates new text based on your seed_text input string. Print the full text with the generated text at the end.\n",
        "# This time you dont get detailed instructions.\n",
        "# Have fun!\n",
        "\n",
        "seed_text"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
